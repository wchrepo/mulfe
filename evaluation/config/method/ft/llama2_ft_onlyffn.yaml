model_name: meta-llama/Llama-2-7b-hf
editor:
  type: FineTuning
  trainable_pattern: 'transformer\.layers\.{layers}\.mlp\.'
  opt_name: AdamW
  opt_kwargs:
    lr: 1e-5
    weight_decay: 0.0
  minibatch_tokens: 1024
  steps: 25
  early_stop: 0.005
  train_mode: False
